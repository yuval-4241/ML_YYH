{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66d616f40293b90d",
   "metadata": {},
   "source": [
    "## lets start by doewload the libareis + random numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "743b5d8b-a878-42bc-a75e-83750818f02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (2.3.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: click in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# התקנת כל הספריות הנדרשות ישירות לתוך ה-Kernel הנוכחי\n",
    "%pip install seaborn matplotlib pandas numpy scikit-learn joblib nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 714 articles.\n",
      "Cleaning columns separately: ['title', 'trail_text', 'tags', 'body']...\n",
      "Final count: 714\n",
      "Sample cleaned Trail Text: yorgos poor things lanthimos reunites emma stone weird kidnapping thriller kim kardashian sarah paulson get right side law ryan murphys story\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pathlib import Path\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# הגדרת Seed לשחזור תוצאות (דרישה מס' 3)\n",
    "np.random.seed(42)\n",
    "\n",
    "# הורדת מילון המילים (אם צריך)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "# הגדרת נתיבים\n",
    "PROJECT_DIR = Path(r\"C:\\Users\\yuval\\Desktop\\לימודים\\ML\")\n",
    "# וודאי שהשורה הזו בקוד שלך נראית ככה:\n",
    "INPUT_FILE = PROJECT_DIR / \"sensed_data.csv\"\n",
    "OUTPUT_FILE = PROJECT_DIR / \"processed_data_separated.csv\" # השם החדש\n",
    "MODEL_DIR = PROJECT_DIR / \"models\"\n",
    "\n",
    "if not MODEL_DIR.exists():\n",
    "    MODEL_DIR.mkdir()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62193adb-bc59-4517-ac88-75c4b17774d9",
   "metadata": {},
   "source": [
    "## 1. Pre-Processing (עיבוד מקדים)\n",
    "בשלב זה ביצענו טיוב נתונים (Data Cleaning) כדי להמיר את הטקסט הגולמי לפורמט אחיד. הפעולות שבוצעו:\n",
    "1. **טיפול בערכים חסרים וכפילויות:** הסרת כתבות ללא כותרת והסרת כפילויות לפי URL.\n",
    "2. **Text Normalization:** המרת כל הטקסט לאותיות קטנות (Lowercase).\n",
    "3. **Noise Reduction:** הסרת תווים מיוחדים, מספרים וסימני פיסוק באמצעות Regex.\n",
    "4. **Stop Words Removal:** הסרת מילות קישור נפוצות (כגון \"the\", \"is\") באמצעות ספריית NLTK, כדי להקטין את המימד ולהתמקד במילים בעלות משמעות."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15cf049a-fbe0-48ab-adea-febe04f89301",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NLTK_STOP_WORDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     10\u001b[39m CUSTOM_STOP_WORDS = {\n\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msays\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msaid\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msay\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtelling\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtold\u001b[39m\u001b[33m'\u001b[39m,  \u001b[38;5;66;03m# פעלים של דיבור (נפוץ בחדשות)\u001b[39;00m\n\u001b[32m     12\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmr\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mms\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmrs\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdr\u001b[39m\u001b[33m'\u001b[39m,                   \u001b[38;5;66;03m# תארים\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mpeople\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mus\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33muk\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mworld\u001b[39m\u001b[33m'\u001b[39m              \u001b[38;5;66;03m# מקומות/אנשים כלליים\u001b[39;00m\n\u001b[32m     18\u001b[39m }\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# 3. איחוד הרשימות לרשימה אחת חזקה\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m FULL_STOP_WORDS = \u001b[43mNLTK_STOP_WORDS\u001b[49m.union(CUSTOM_STOP_WORDS)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# טעינת הנתונים\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m INPUT_FILE.exists():\n",
      "\u001b[31mNameError\u001b[39m: name 'NLTK_STOP_WORDS' is not defined"
     ]
    }
   ],
   "source": [
    "def clean_text_noise(text):\n",
    "    \"\"\"פונקציית ניקוי: Lowercase, Regex, Stopwords\"\"\"\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    filtered = [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "    return \" \".join(filtered)\n",
    "  # תוספת חשובה: מילים ספציפיות שראינו שצריך להעיף ---\n",
    "CUSTOM_STOP_WORDS = {\n",
    "    'says', 'said', 'say', 'telling', 'told',  # פעלים של דיבור (נפוץ בחדשות)\n",
    "    'mr', 'ms', 'mrs', 'dr',                   # תארים\n",
    "    'one', 'two', 'three',                     # מספרים במילים\n",
    "    'would', 'could', 'should', 'may', 'might', # פעלים מודאליים חלשים\n",
    "    'also', 'even', 'like', 'get', 'go',       # מילים כלליות מדי\n",
    "    'new', 'news', 'report', 'time', 'year',   # מילים שנפוצות בכל העיתון\n",
    "    'people', 'us', 'uk', 'world'              # מקומות/אנשים כלליים\n",
    "}\n",
    "\n",
    "# 3. איחוד הרשימות לרשימה אחת חזקה\n",
    "FULL_STOP_WORDS = NLTK_STOP_WORDS.union(CUSTOM_STOP_WORDS)\n",
    "\n",
    "# טעינת הנתונים\n",
    "if INPUT_FILE.exists():\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    print(f\"Loaded {len(df)} articles.\")\n",
    "\n",
    "    # 1. ניקוי שורות בסיסי (כפילויות וחסרים)\n",
    "    df = df.dropna(subset=['title'])\n",
    "    df = df.drop_duplicates(subset=['url'])\n",
    "\n",
    "    # 2. ניקוי שם הכותב\n",
    "    df['author'] = df['author'].fillna('unknown').astype(str)\n",
    "    df['author'] = df['author'].apply(lambda x: x.lower().replace('by ', '').strip())\n",
    "\n",
    "    # 3. ניקוי עמודות הטקסט (בנפרד!)\n",
    "    text_columns = ['title', 'trail_text', 'tags', 'body']\n",
    "    print(f\"Cleaning columns separately: {text_columns}...\")\n",
    "\n",
    "    for col in text_columns:\n",
    "        # מילוי ערכים חסרים\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna('')\n",
    "            df[col] = df[col].apply(clean_text_noise)\n",
    "\n",
    "    # 4. סינון שורות ללא תקציר (Trail Text)\n",
    "    df = df[df['trail_text'].str.len() > 2]\n",
    "\n",
    "    # הצגת דוגמה לניקוי\n",
    "    print(f\"Final count: {len(df)}\")\n",
    "    print(\"Sample cleaned Trail Text:\", df['trail_text'].iloc[0])\n",
    "    \n",
    "    # שמירה זמנית (כדי שהשלב הבא יוכל להשתמש בזה)\n",
    "    df.to_csv(OUTPUT_FILE, index=False)\n",
    "else:\n",
    "    print(f\"Error: {INPUT_FILE} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91da0f64-2e9d-4ebb-a0d2-d85a9ec84344",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction (חילוץ מאפיינים)\n",
    "בשלב זה נמיר את המידע הטקסטואלי למאפיינים מספריים בשלוש שיטות:\n",
    "1.  **Trail Text (תקציר):** שימוש ב-TF-IDF כדי לזהות מילות מפתח בתקציר הכתבה (1,000 המילים המובילות).\n",
    "2.  **Tags (תגיות):** שימוש ב-TF-IDF על התגיות המקצועיות (500 התגיות המובילות).\n",
    "3.  **Authors (כותבים):** זיהוי כותבים \"מומחים\" (Specialists) וייצוגם כמאפיינים בינאריים."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d68b45a-b619-4fa1-b53c-7e06444f3602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_top_features(df, vectorizer, tfidf_matrix, feature_type_name):\n",
    "    \"\"\"מדפיסה את המילים/תגיות החזקות ביותר בכל קטגוריה\"\"\"\n",
    "    print(f\"\\n--- Top Significant {feature_type_name} per Category ---\")\n",
    "    unique_labels = df['label'].unique()\n",
    "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "    for label in unique_labels:\n",
    "        indices = df.index[df['label'] == label].tolist()\n",
    "        if not indices: continue\n",
    "\n",
    "        class_matrix = tfidf_matrix[indices]\n",
    "        mean_scores = np.array(class_matrix.mean(axis=0)).flatten()\n",
    "        top_indices = mean_scores.argsort()[-10:][::-1]\n",
    "\n",
    "        print(f\"Category: {label.upper()}\")\n",
    "        results = [f\"{feature_names[i]}\" for i in top_indices]\n",
    "        print(\", \".join(results))\n",
    "\n",
    "def get_specialist_authors(df, min_articles=3):\n",
    "    \"\"\"מחזירה רשימת כותבים שכתבו לפחות 3 כתבות ורק בנושא אחד\"\"\"\n",
    "    print(f\"\\nAnalyzing Author Specialization...\")\n",
    "    df_clean = df[~df['author'].isin(['unknown', 'unknown author', 'guardian staff'])]\n",
    "    author_matrix = pd.crosstab(df_clean['author'], df_clean['label'])\n",
    "    active_authors = author_matrix[author_matrix.sum(axis=1) >= min_articles]\n",
    "    is_specialist = (active_authors > 0).sum(axis=1) == 1\n",
    "    specialist_authors = active_authors[is_specialist].index.tolist()\n",
    "    print(f\"Found {len(specialist_authors)} specialist authors.\")\n",
    "    return specialist_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b5321f8-4d57-4dd4-8eb6-4d93690e56fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 714 articles for Feature Extraction.\n",
      "\n",
      "1. Extracting Trail-Text Features (Top 1000)...\n",
      "\n",
      "--- Top Significant Trail-Words per Category ---\n",
      "Category: CULTURE\n",
      "the, and, of, to, in, night, hosts, late, for, trump\n",
      "Category: NEWS\n",
      "the, to, of, and, in, plus, from, for, is, as\n",
      "Category: OPINION\n",
      "the, says, of, and, to, is, it, guardian, columnist, editorial\n",
      "Category: SPORT\n",
      "the, in, to, england, of, and, after, with, for, his\n",
      "\n",
      "2. Extracting Tag Features (Top 500)...\n",
      "\n",
      "--- Top Significant Tags per Category ---\n",
      "Category: CULTURE\n",
      "culture, television, comedy, tv, saturday, news, features, and, music, arts\n",
      "Category: NEWS\n",
      "news, uk, the, weather, long, read, weekly, guardian, section, main\n",
      "Category: OPINION\n",
      "opinion, comment, uk, us, news, of, politics, matters, journal, article\n",
      "Category: SPORT\n",
      "sport, cricket, team, rugby, australia, union, ashes, the, news, newsletter\n"
     ]
    }
   ],
   "source": [
    "# טעינת הקובץ המעובד (המופרד)\n",
    "if INPUT_FILE.exists():\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    df['trail_text'] = df['trail_text'].fillna('')\n",
    "    df['tags'] = df['tags'].fillna('')\n",
    "    df['author'] = df['author'].fillna('unknown')\n",
    "    print(f\"Loaded {len(df)} articles for Feature Extraction.\")\n",
    "\n",
    "    # --- A: Trail Text Features ---\n",
    "    print(\"\\n1. Extracting Trail-Text Features (Top 1000)...\")\n",
    "    trail_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    X_trail = trail_vectorizer.fit_transform(df['trail_text'])\n",
    "    \n",
    "    # הצגת הניתוח (כדי לראות שהמילים הגיוניות)\n",
    "    analyze_top_features(df, trail_vectorizer, X_trail, \"Trail-Words\")\n",
    "    \n",
    "    trail_features = pd.DataFrame(X_trail.toarray(), columns=[f\"trail_{w}\" for w in trail_vectorizer.get_feature_names_out()])\n",
    "    joblib.dump(trail_vectorizer, MODEL_DIR / \"tfidf_trail.pkl\")\n",
    "\n",
    "    # --- B: Tag Features ---\n",
    "    print(\"\\n2. Extracting Tag Features (Top 500)...\")\n",
    "    tags_vectorizer = TfidfVectorizer(max_features=500)\n",
    "    X_tags = tags_vectorizer.fit_transform(df['tags'])\n",
    "    \n",
    "    # הצגת הניתוח\n",
    "    analyze_top_features(df, tags_vectorizer, X_tags, \"Tags\")\n",
    "    \n",
    "    tags_features = pd.DataFrame(X_tags.toarray(), columns=[f\"tag_{w}\" for w in tags_vectorizer.get_feature_names_out()])\n",
    "    joblib.dump(tags_vectorizer, MODEL_DIR / \"tfidf_tags.pkl\")\n",
    "else:\n",
    "    print(\"Error: Input file not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "487d9d99-c9c4-439e-b729-d99bb90f293a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Extracting Specialist Author Features...\n",
      "\n",
      "Analyzing Author Specialization...\n",
      "Found 55 specialist authors.\n",
      "\n",
      "4. Combining all features...\n",
      "\n",
      "Success! Final dataset saved to: C:\\Users\\yuval\\Desktop\\לימודים\\ML\\dataset_features_final.csv\n",
      "Dataset Dimensions: (714, 1556)\n",
      " - Trail Text features: 1000\n",
      " - Tag features: 500\n",
      " - Author features: 55\n"
     ]
    }
   ],
   "source": [
    "if INPUT_FILE.exists():\n",
    "    # --- C: Author Features ---\n",
    "    print(\"\\n3. Extracting Specialist Author Features...\")\n",
    "    selected_authors = get_specialist_authors(df, min_articles=3)\n",
    "    joblib.dump(selected_authors, MODEL_DIR / \"authors_list.pkl\")\n",
    "    \n",
    "    author_features = pd.DataFrame()\n",
    "    for author in selected_authors:\n",
    "        col_name = f\"auth_{author.replace(' ', '_')}\"\n",
    "        author_features[col_name] = df['author'].apply(lambda x: 1 if x == author else 0)\n",
    "\n",
    "    # --- D: Combine and Save ---\n",
    "    print(\"\\n4. Combining all features...\")\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    trail_features.reset_index(drop=True, inplace=True)\n",
    "    tags_features.reset_index(drop=True, inplace=True)\n",
    "    author_features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    final_dataset = pd.concat([trail_features, tags_features, author_features], axis=1)\n",
    "    final_dataset['label'] = df['label']\n",
    "    \n",
    "    final_dataset.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"\\nSuccess! Final dataset saved to: {OUTPUT_FILE}\")\n",
    "    print(f\"Dataset Dimensions: {final_dataset.shape}\")\n",
    "    print(f\" - Trail Text features: {trail_features.shape[1]}\")\n",
    "    print(f\" - Tag features: {tags_features.shape[1]}\")\n",
    "    print(f\" - Author features: {author_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea28bdca-a287-4c49-b2bc-26d745a8dfbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
