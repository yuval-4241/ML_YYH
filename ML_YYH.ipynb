{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66d616f40293b90d",
   "metadata": {},
   "source": [
    "## lets start by doewload the libareis + random numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "743b5d8b-a878-42bc-a75e-83750818f02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (2.3.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: click in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# התקנת כל הספריות הנדרשות ישירות לתוך ה-Kernel הנוכחי\n",
    "%pip install seaborn matplotlib pandas numpy scikit-learn joblib nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 714 articles.\n",
      "Cleaning columns separately: ['title', 'trail_text', 'tags', 'body']...\n",
      "Final count: 714\n",
      "Sample cleaned Trail Text: yorgos poor things lanthimos reunites emma stone weird kidnapping thriller kim kardashian sarah paulson get right side law ryan murphys story\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pathlib import Path\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# הגדרת Seed לשחזור תוצאות (דרישה מס' 3)\n",
    "np.random.seed(42)\n",
    "\n",
    "# הורדת מילון המילים (אם צריך)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "# הגדרת נתיבים\n",
    "PROJECT_DIR = Path(r\"C:\\Users\\yuval\\Desktop\\לימודים\\ML\")\n",
    "# וודאי שהשורה הזו בקוד שלך נראית ככה:\n",
    "INPUT_FILE = PROJECT_DIR / \"sensed_data.csv\"\n",
    "OUTPUT_FILE = PROJECT_DIR / \"processed_data_separated.csv\" # השם החדש\n",
    "MODEL_DIR = PROJECT_DIR / \"models\"\n",
    "\n",
    "if not MODEL_DIR.exists():\n",
    "    MODEL_DIR.mkdir()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62193adb-bc59-4517-ac88-75c4b17774d9",
   "metadata": {},
   "source": [
    "## 1. Pre-Processing (עיבוד מקדים וניקוי)\n",
    "\n",
    "בשלב זה ביצענו טיוב נתונים (Data Cleaning) כדי להפוך את הטקסט הגולמי לפורמט נקי ואחיד שמתאים למודל למידה.\n",
    "תהליך הניקוי בוצע על כל עמודה רלוונטית בנפרד (Trail Text, Tags, Authors) וכלל את השלבים הבאים:\n",
    "\n",
    "1.  **Text Normalization:** המרת כל הטקסט לאותיות קטנות (Lowercase) כדי למנוע כפילויות משמעות.\n",
    "2.  **Noise Reduction:** שימוש בביטויים רגולריים (Regex) להסרת כל תו שאינו אות (מספרים, סימני פיסוק, תווים מיוחדים).\n",
    "3.  **Tokenization:** פירוק המשפט למילים בודדות (Tokens) לצורך סינון.\n",
    "4.  **Stop Words Removal:** הסרת מילות קישור נפוצות (כגון \"the\", \"is\") באמצעות ספריית NLTK.\n",
    "    * **Custom Stop Words:** הוספנו רשימה ידנית של מילים נפוצות בחדשות שאינן תורמות לסיווג (כגון \"said\", \"mr\", \"report\") והסרנו גם אותן.\n",
    "5.  **Reassembly:** הרכבת המילים שנשארו חזרה למחרוזת טקסט נקייה.\n",
    "6.  **Data Cleaning:** הסרת כפילויות (לפי URL) ומחיקת שורות עם מידע חסר.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15cf049a-fbe0-48ab-adea-febe04f89301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 862 articles.\n",
      "Cleaning columns separately: ['title', 'trail_text', 'tags', 'body']...\n",
      "Final count: 862\n",
      "Sample cleaned Trail Text: yorgos poor things lanthimos reunites emma stone weird kidnapping thriller kim kardashian sarah paulson get right side law ryan murphys story\n"
     ]
    }
   ],
   "source": [
    "def clean_text_noise(text):\n",
    "    \"\"\"פונקציית ניקוי: Lowercase, Regex, Stopwords\"\"\"\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    filtered = [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "    return \" \".join(filtered)\n",
    "  # תוספת חשובה: מילים ספציפיות שראינו שצריך להעיף ---\n",
    "nltk.download('stopwords', quiet=True)\n",
    "NLTK_STOP_WORDS = set(stopwords.words('english'))\n",
    "CUSTOM_STOP_WORDS = {\n",
    "    'says', 'said', 'says', 'telling', 'told',  # פעלים של דיבור (נפוץ בחדשות)\n",
    "    'mr', 'ms', 'mrs', 'dr', 'of','is','in'                  # תארים\n",
    "    'one', 'two', 'three',                     # מספרים במילים\n",
    "    'would', 'could', 'should', 'may', 'might', # פעלים מודאליים חלשים\n",
    "    'also', 'even', 'like', 'get', 'go',       # מילים כלליות מדי\n",
    "    'new', 'news', 'report', 'time', 'year',   # מילים שנפוצות בכל העיתון\n",
    "    'people', 'us', 'uk', 'world'              # מקומות/אנשים כלליים\n",
    "}\n",
    "\n",
    "# 3. איחוד הרשימות לרשימה אחת חזקה\n",
    "FULL_STOP_WORDS = NLTK_STOP_WORDS.union(CUSTOM_STOP_WORDS)\n",
    "\n",
    "# טעינת הנתונים\n",
    "if INPUT_FILE.exists():\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    print(f\"Loaded {len(df)} articles.\")\n",
    "\n",
    "    # 1. ניקוי שורות בסיסי (כפילויות וחסרים)\n",
    "    df = df.dropna(subset=['title'])\n",
    "    df = df.drop_duplicates(subset=['url'])\n",
    "\n",
    "    # 2. ניקוי שם הכותב\n",
    "    df['author'] = df['author'].fillna('unknown').astype(str)\n",
    "    df['author'] = df['author'].apply(lambda x: x.lower().replace('by ', '').strip())\n",
    "\n",
    "    # 3. ניקוי עמודות הטקסט (בנפרד!)\n",
    "    text_columns = ['title', 'trail_text', 'tags', 'body']\n",
    "    print(f\"Cleaning columns separately: {text_columns}...\")\n",
    "\n",
    "    for col in text_columns:\n",
    "        # מילוי ערכים חסרים\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna('')\n",
    "            df[col] = df[col].apply(clean_text_noise)\n",
    "\n",
    "    # 4. סינון שורות ללא תקציר (Trail Text)\n",
    "    df = df[df['trail_text'].str.len() > 2]\n",
    "\n",
    "    # הצגת דוגמה לניקוי\n",
    "    print(f\"Final count: {len(df)}\")\n",
    "    print(\"Sample cleaned Trail Text:\", df['trail_text'].iloc[0])\n",
    "    # הצגת השוואה בין המקור לניקוי עבור 5 הכתבות הראשונות\n",
    " \n",
    "    \n",
    "    # שמירה זמנית (כדי שהשלב הבא יוכל להשתמש בזה)\n",
    "    df.to_csv(OUTPUT_FILE, index=False)\n",
    "else:\n",
    "    print(f\"Error: {INPUT_FILE} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91da0f64-2e9d-4ebb-a0d2-d85a9ec84344",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction (חילוץ מאפיינים)\n",
    "בשלב זה נמיר את המידע הטקסטואלי למאפיינים מספריים בשלוש שיטות:\n",
    "1.  **Trail Text (תקציר):** שימוש ב-TF-IDF כדי לזהות מילות מפתח בתקציר הכתבה (1,000 המילים המובילות).\n",
    "2.  **Tags (תגיות):** שימוש ב-TF-IDF על התגיות המקצועיות (500 התגיות המובילות).\n",
    "3.  **Authors (כותבים):** זיהוי כותבים \"מומחים\" (Specialists) וייצוגם כמאפיינים בינאריים.\n",
    "\n",
    " בחירת גודל המילון (Quantitative Selection via Frequency): עבור ייצוג ה-TF-IDF, הגבלנו את כמות המאפיינים (מילים) ל-1,000 עבור הטקסט (Trail Text) ו-500 עבור התגיות (Tags). הבחירה במאפיינים אלו נעשתה על בסיס שכיחות בקורפוס (Corpus Frequency) ולא על בסיס ציון TF-IDF.\n",
    "   על המילון שנבחר על פי שכיחות ביצענו TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d68b45a-b619-4fa1-b53c-7e06444f3602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_top_features(df, vectorizer, tfidf_matrix, feature_type_name):\n",
    "    \"\"\"מדפיסה את המילים/תגיות החזקות ביותר בכל קטגוריה\"\"\"\n",
    "    print(f\"\\n--- Top Significant {feature_type_name} per Category ---\")\n",
    "    unique_labels = df['label'].unique()\n",
    "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "    for label in unique_labels:\n",
    "        indices = df.index[df['label'] == label].tolist()\n",
    "        if not indices: continue\n",
    "\n",
    "        class_matrix = tfidf_matrix[indices]\n",
    "        mean_scores = np.array(class_matrix.mean(axis=0)).flatten()\n",
    "        top_indices = mean_scores.argsort()[-10:][::-1]\n",
    "\n",
    "        print(f\"Category: {label.upper()}\")\n",
    "        results = [f\"{feature_names[i]}\" for i in top_indices]\n",
    "        print(\", \".join(results))\n",
    "\n",
    "def get_specialist_authors(df, min_articles=3):\n",
    "    \"\"\"מחזירה רשימת כותבים שכתבו לפחות 3 כתבות ורק בנושא אחד\"\"\"\n",
    "    print(f\"\\nAnalyzing Author Specialization...\")\n",
    "    df_clean = df[~df['author'].isin(['unknown', 'unknown author', 'guardian staff'])]\n",
    "    author_matrix = pd.crosstab(df_clean['author'], df_clean['label'])\n",
    "    active_authors = author_matrix[author_matrix.sum(axis=1) >= min_articles]\n",
    "    is_specialist = (active_authors > 0).sum(axis=1) == 1\n",
    "    specialist_authors = active_authors[is_specialist].index.tolist()\n",
    "    print(f\"Found {len(specialist_authors)} specialist authors.\")\n",
    "    return specialist_authors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a559c30f-e78f-44ca-88d9-a5739ad4ed09",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "  מניעת דליפת מידע (Data Leakage Prevention - Qualitative): במסגרת הסינון האיכותני (Qualitative Filtering), זיהינו כי קיים סיכון חמור ל-Label Leakage בנתוני התגיות.\n",
    "\n",
    "הבעיה: תגיות רבות באתר The Guardian מכילות באופן מפורש את שם המדור (למשל, כתבה ממדור ספורט תכלול תגית בשם \"Sport\" או \"Football Sport\"). מצב זה גורם למודל \"לראות את התשובה\" בתוך המאפיינים (Feature Set), מה שמוביל לביצועים מושלמים באופן מלאכותי באימון אך לכישלון בעולם האמיתי.\n",
    "\n",
    "הפתרון: יצרנו רשימת חסימה (Stop List) ייעודית שכללה את שמות כל הקטגוריות (News, Sport, Culture, Opinion, UK, World). הסרנו מילים אלו באופן יזום הן מהטקסט והן מהתגיות לפני חישוב ה-TF-IDF, כדי לאלץ את המודל ללמוד מתוך התוכן המהותי ולא להסתמך על המטא-דאטה שמסגיר את הקטגוריה.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b5321f8-4d57-4dd4-8eb6-4d93690e56fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Extracting Trail-Text Features (Top 1000)...\n",
      "\n",
      "2. Extracting Tag Features (Top 500) - With Leakage Prevention...\n",
      "\n",
      "--- Top Significant Tags (Cleaned) per Category ---\n",
      "Category: CULTURE\n",
      "television, comedy, saturday, features, arts, music, article, radio, art, film\n",
      "Category: NEWS\n",
      "long, weather, read, weekly, section, main, guardian, features, crunch, newsletters\n",
      "Category: OPINION\n",
      "comment, matters, politics, journal, article, trump, labour, donald, guardian, health\n",
      "Category: SPORT\n",
      "cricket, team, australia, rugby, ashes, union, features, sports, newsletter, guardian\n",
      "\n",
      "--- Top Significant Trail-Words per Category ---\n",
      "Category: CULTURE\n",
      "latenight, hosts, trumps, new, discuss, discussed, weeks, shutdown, star, years\n",
      "Category: NEWS\n",
      "plus, read, years, data, days, need, pieces, climate, weekend, favourite\n",
      "Category: OPINION\n",
      "says, guardian, columnist, editorial, government, new, asks, writes, people, president\n",
      "Category: SPORT\n",
      "england, ashes, australia, test, win, blacks, englands, team, season, victory\n",
      "\n",
      "2. Extracting Tag Features (Top 500)...\n",
      "\n",
      "--- Top Significant Tags per Category ---\n",
      "Category: CULTURE\n",
      "culture, television, comedy, saturday, features, news, arts, music, article, radio\n",
      "Category: NEWS\n",
      "news, long, weather, read, weekly, guardian, main, section, features, crunch\n",
      "Category: OPINION\n",
      "opinion, comment, news, politics, matters, journal, article, trump, labour, donald\n",
      "Category: SPORT\n",
      "sport, cricket, team, rugby, australia, union, ashes, news, sports, newsletter\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\yuval\\\\Desktop\\\\לימודים\\\\ML\\\\models\\\\tfidf_tags.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- A. Trail Text Features ---\n",
    "print(\"\\n1. Extracting Trail-Text Features (Top 1000)...\")\n",
    "# --- הגדרת מילים אסורות (שמות הקטגוריות) למניעת Data Leakage ---\n",
    "# אנחנו מוסיפים את שמות הקטגוריות לרשימת ה-Stop Words\n",
    "# כדי שהמודל לא יקבל את התשובה (\"Sport\") בתוך התגיות\n",
    "category_names = {'sport', 'news', 'culture', 'opinion', 'commentisfree', 'uk', 'world'} \n",
    "# הוספנו גם uk ו-world כי הן מופיעות המון בחדשות כחלק מהשם\n",
    "\n",
    "# איחוד עם הרשימה האנגלית הסטנדרטית\n",
    "tags_stop_words = list(stopwords.words('english')) + list(category_names)\n",
    "\n",
    "# --- B. Tag Features (Updated) ---\n",
    "print(\"\\n2. Extracting Tag Features (Top 500) - With Leakage Prevention...\")\n",
    "\n",
    "# שימוש ברשימה המעודכנת בתוך stop_words\n",
    "tags_vectorizer = TfidfVectorizer(max_features=500, stop_words=tags_stop_words)\n",
    "\n",
    "X_tags = tags_vectorizer.fit_transform(df['tags'])\n",
    "\n",
    "# הצגת הניתוח (כדי לוודא שהמילים האסורות נעלמו)\n",
    "analyze_top_features(df, tags_vectorizer, X_tags, \"Tags (Cleaned)\")\n",
    "\n",
    "tags_features = pd.DataFrame(\n",
    "    X_tags.toarray(), \n",
    "    columns=[f\"tag_{w}\" for w in tags_vectorizer.get_feature_names_out()]\n",
    ")\n",
    "joblib.dump(tags_vectorizer, MODEL_DIR / \"tfidf_tags.pkl\")\n",
    "\n",
    "# התיקון: הוספנו stop_words='english' כדי להעיף את the, and, of\n",
    "trail_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "\n",
    "# אם את רוצה להוסיף גם את המילים הספציפיות שלך (כמו says), אפשר להעביר רשימה (אופציונלי)\n",
    "# אבל stop_words='english' יעשה את רוב העבודה הקשה\n",
    "X_trail = trail_vectorizer.fit_transform(df['trail_text'])\n",
    "\n",
    "# הצגת הניתוח (כדי לראות שהמילים הגיוניות עכשיו)\n",
    "analyze_top_features(df, trail_vectorizer, X_trail, \"Trail-Words\")\n",
    "\n",
    "trail_features = pd.DataFrame(\n",
    "    X_trail.toarray(), \n",
    "    columns=[f\"trail_{w}\" for w in trail_vectorizer.get_feature_names_out()]\n",
    ")\n",
    "joblib.dump(trail_vectorizer, MODEL_DIR / \"tfidf_trail.pkl\")\n",
    "\n",
    "# --- B. Tag Features ---\n",
    "print(\"\\n2. Extracting Tag Features (Top 500)...\")\n",
    "# גם כאן כדאי להוסיף ליתר ביטחון, למרות שבתגיות זה פחות קריטי\n",
    "tags_vectorizer = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "X_tags = tags_vectorizer.fit_transform(df['tags'])\n",
    "\n",
    "# הצגת הניתוח\n",
    "analyze_top_features(df, tags_vectorizer, X_tags, \"Tags\")\n",
    "\n",
    "tags_features = pd.DataFrame(\n",
    "    X_tags.toarray(), \n",
    "    columns=[f\"tag_{w}\" for w in tags_vectorizer.get_feature_names_out()]\n",
    ")\n",
    "joblib.dump(tags_vectorizer, MODEL_DIR / \"tfidf_tags.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "487d9d99-c9c4-439e-b729-d99bb90f293a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Extracting Specialist Author Features...\n",
      "\n",
      "Analyzing Author Specialization...\n",
      "Found 68 specialist authors.\n",
      "\n",
      "4. Combining all features...\n",
      "\n",
      "Success! Final dataset saved to: C:\\Users\\yuval\\Desktop\\לימודים\\ML\\dataset_features_final.csv\n",
      "Dataset Dimensions: (862, 1569)\n",
      " - Trail Text features: 1000\n",
      " - Tag features: 500\n",
      " - Author features: 68\n"
     ]
    }
   ],
   "source": [
    "if INPUT_FILE.exists():\n",
    "    # --- C: Author Features ---\n",
    "    print(\"\\n3. Extracting Specialist Author Features...\")\n",
    "    selected_authors = get_specialist_authors(df, min_articles=3)\n",
    "    joblib.dump(selected_authors, MODEL_DIR / \"authors_list.pkl\")\n",
    "    \n",
    "    author_features = pd.DataFrame()\n",
    "    for author in selected_authors:\n",
    "        col_name = f\"auth_{author.replace(' ', '_')}\"\n",
    "        author_features[col_name] = df['author'].apply(lambda x: 1 if x == author else 0)\n",
    "\n",
    "    # --- D: Combine and Save ---\n",
    "    print(\"\\n4. Combining all features...\")\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    trail_features.reset_index(drop=True, inplace=True)\n",
    "    tags_features.reset_index(drop=True, inplace=True)\n",
    "    author_features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    final_dataset = pd.concat([trail_features, tags_features, author_features], axis=1)\n",
    "    final_dataset['label'] = df['label']\n",
    "    \n",
    "    final_dataset.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"\\nSuccess! Final dataset saved to: {OUTPUT_FILE}\")\n",
    "    print(f\"Dataset Dimensions: {final_dataset.shape}\")\n",
    "    print(f\" - Trail Text features: {trail_features.shape[1]}\")\n",
    "    print(f\" - Tag features: {tags_features.shape[1]}\")\n",
    "    print(f\" - Author features: {author_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea28bdca-a287-4c49-b2bc-26d745a8dfbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
