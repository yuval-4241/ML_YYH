{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66d616f40293b90d",
   "metadata": {},
   "source": [
    "## lets start by doewload the libareis + random numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "743b5d8b-a878-42bc-a75e-83750818f02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (2.3.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: click in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yuval\\pycharmprojects\\ml_yy\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# התקנת כל הספריות הנדרשות ישירות לתוך ה-Kernel הנוכחי\n",
    "%pip install seaborn matplotlib pandas numpy scikit-learn joblib nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 714 articles.\n",
      "Cleaning columns separately: ['title', 'trail_text', 'tags', 'body']...\n",
      "Final count: 714\n",
      "Sample cleaned Trail Text: yorgos poor things lanthimos reunites emma stone weird kidnapping thriller kim kardashian sarah paulson get right side law ryan murphys story\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pathlib import Path\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# הגדרת Seed לשחזור תוצאות (דרישה מס' 3)\n",
    "np.random.seed(42)\n",
    "\n",
    "# הורדת מילון המילים (אם צריך)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "# הגדרת נתיבים\n",
    "PROJECT_DIR = Path(r\"C:\\Users\\yuval\\Desktop\\לימודים\\ML\")\n",
    "# וודאי שהשורה הזו בקוד שלך נראית ככה:\n",
    "INPUT_FILE = PROJECT_DIR / \"sensed_data.csv\"\n",
    "OUTPUT_FILE = PROJECT_DIR / \"processed_data_separated.csv\" # השם החדש\n",
    "MODEL_DIR = PROJECT_DIR / \"models\"\n",
    "\n",
    "if not MODEL_DIR.exists():\n",
    "    MODEL_DIR.mkdir()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62193adb-bc59-4517-ac88-75c4b17774d9",
   "metadata": {},
   "source": [
    "## 1. Pre-Processing (עיבוד מקדים)\n",
    "בשלב זה ביצענו טיוב נתונים (Data Cleaning) כדי להמיר את הטקסט הגולמי לפורמט אחיד. הפעולות שבוצעו:\n",
    "1. **טיפול בערכים חסרים וכפילויות:** הסרת כתבות ללא כותרת והסרת כפילויות לפי URL.\n",
    "2. **Text Normalization:** המרת כל הטקסט לאותיות קטנות (Lowercase).\n",
    "3. **Noise Reduction:** הסרת תווים מיוחדים, מספרים וסימני פיסוק באמצעות Regex.\n",
    "4. **Stop Words Removal:** הסרת מילות קישור נפוצות (כגון \"the\", \"is\") באמצעות ספריית NLTK, כדי להקטין את המימד ולהתמקד במילים בעלות משמעות."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15cf049a-fbe0-48ab-adea-febe04f89301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 714 articles.\n",
      "Cleaning columns separately: ['title', 'trail_text', 'tags', 'body']...\n",
      "Final count: 714\n",
      "Sample cleaned Trail Text: yorgos poor things lanthimos reunites emma stone weird kidnapping thriller kim kardashian sarah paulson get right side law ryan murphys story\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['clean_trail_text'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSample cleaned Trail Text:\u001b[39m\u001b[33m\"\u001b[39m, df[\u001b[33m'\u001b[39m\u001b[33mtrail_text\u001b[39m\u001b[33m'\u001b[39m].iloc[\u001b[32m0\u001b[39m])\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# הצגת השוואה בין המקור לניקוי עבור 5 הכתבות הראשונות\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrail_text\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mclean_trail_text\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.head())\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# שמירה זמנית (כדי שהשלב הבא יוכל להשתמש בזה)\u001b[39;00m\n\u001b[32m     58\u001b[39m df.to_csv(OUTPUT_FILE, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\ML_YY\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4119\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4118\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4119\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4121\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\ML_YY\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\ML_YY\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['clean_trail_text'] not in index\""
     ]
    }
   ],
   "source": [
    "def clean_text_noise(text):\n",
    "    \"\"\"פונקציית ניקוי: Lowercase, Regex, Stopwords\"\"\"\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    filtered = [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "    return \" \".join(filtered)\n",
    "  # תוספת חשובה: מילים ספציפיות שראינו שצריך להעיף ---\n",
    "nltk.download('stopwords', quiet=True)\n",
    "NLTK_STOP_WORDS = set(stopwords.words('english'))\n",
    "CUSTOM_STOP_WORDS = {\n",
    "    'says', 'said', 'says', 'telling', 'told',  # פעלים של דיבור (נפוץ בחדשות)\n",
    "    'mr', 'ms', 'mrs', 'dr', 'of','is','in'                  # תארים\n",
    "    'one', 'two', 'three',                     # מספרים במילים\n",
    "    'would', 'could', 'should', 'may', 'might', # פעלים מודאליים חלשים\n",
    "    'also', 'even', 'like', 'get', 'go',       # מילים כלליות מדי\n",
    "    'new', 'news', 'report', 'time', 'year',   # מילים שנפוצות בכל העיתון\n",
    "    'people', 'us', 'uk', 'world'              # מקומות/אנשים כלליים\n",
    "}\n",
    "\n",
    "# 3. איחוד הרשימות לרשימה אחת חזקה\n",
    "FULL_STOP_WORDS = NLTK_STOP_WORDS.union(CUSTOM_STOP_WORDS)\n",
    "\n",
    "# טעינת הנתונים\n",
    "if INPUT_FILE.exists():\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    print(f\"Loaded {len(df)} articles.\")\n",
    "\n",
    "    # 1. ניקוי שורות בסיסי (כפילויות וחסרים)\n",
    "    df = df.dropna(subset=['title'])\n",
    "    df = df.drop_duplicates(subset=['url'])\n",
    "\n",
    "    # 2. ניקוי שם הכותב\n",
    "    df['author'] = df['author'].fillna('unknown').astype(str)\n",
    "    df['author'] = df['author'].apply(lambda x: x.lower().replace('by ', '').strip())\n",
    "\n",
    "    # 3. ניקוי עמודות הטקסט (בנפרד!)\n",
    "    text_columns = ['title', 'trail_text', 'tags', 'body']\n",
    "    print(f\"Cleaning columns separately: {text_columns}...\")\n",
    "\n",
    "    for col in text_columns:\n",
    "        # מילוי ערכים חסרים\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna('')\n",
    "            df[col] = df[col].apply(clean_text_noise)\n",
    "\n",
    "    # 4. סינון שורות ללא תקציר (Trail Text)\n",
    "    df = df[df['trail_text'].str.len() > 2]\n",
    "\n",
    "    # הצגת דוגמה לניקוי\n",
    "    print(f\"Final count: {len(df)}\")\n",
    "    print(\"Sample cleaned Trail Text:\", df['trail_text'].iloc[0])\n",
    "    # הצגת השוואה בין המקור לניקוי עבור 5 הכתבות הראשונות\n",
    "    print(df[['trail_text', 'clean_trail_text']].head())\n",
    "    \n",
    "    # שמירה זמנית (כדי שהשלב הבא יוכל להשתמש בזה)\n",
    "    df.to_csv(OUTPUT_FILE, index=False)\n",
    "else:\n",
    "    print(f\"Error: {INPUT_FILE} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91da0f64-2e9d-4ebb-a0d2-d85a9ec84344",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction (חילוץ מאפיינים)\n",
    "בשלב זה נמיר את המידע הטקסטואלי למאפיינים מספריים בשלוש שיטות:\n",
    "1.  **Trail Text (תקציר):** שימוש ב-TF-IDF כדי לזהות מילות מפתח בתקציר הכתבה (1,000 המילים המובילות).\n",
    "2.  **Tags (תגיות):** שימוש ב-TF-IDF על התגיות המקצועיות (500 התגיות המובילות).\n",
    "3.  **Authors (כותבים):** זיהוי כותבים \"מומחים\" (Specialists) וייצוגם כמאפיינים בינאריים."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d68b45a-b619-4fa1-b53c-7e06444f3602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_top_features(df, vectorizer, tfidf_matrix, feature_type_name):\n",
    "    \"\"\"מדפיסה את המילים/תגיות החזקות ביותר בכל קטגוריה\"\"\"\n",
    "    print(f\"\\n--- Top Significant {feature_type_name} per Category ---\")\n",
    "    unique_labels = df['label'].unique()\n",
    "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "    for label in unique_labels:\n",
    "        indices = df.index[df['label'] == label].tolist()\n",
    "        if not indices: continue\n",
    "\n",
    "        class_matrix = tfidf_matrix[indices]\n",
    "        mean_scores = np.array(class_matrix.mean(axis=0)).flatten()\n",
    "        top_indices = mean_scores.argsort()[-10:][::-1]\n",
    "\n",
    "        print(f\"Category: {label.upper()}\")\n",
    "        results = [f\"{feature_names[i]}\" for i in top_indices]\n",
    "        print(\", \".join(results))\n",
    "\n",
    "def get_specialist_authors(df, min_articles=3):\n",
    "    \"\"\"מחזירה רשימת כותבים שכתבו לפחות 3 כתבות ורק בנושא אחד\"\"\"\n",
    "    print(f\"\\nAnalyzing Author Specialization...\")\n",
    "    df_clean = df[~df['author'].isin(['unknown', 'unknown author', 'guardian staff'])]\n",
    "    author_matrix = pd.crosstab(df_clean['author'], df_clean['label'])\n",
    "    active_authors = author_matrix[author_matrix.sum(axis=1) >= min_articles]\n",
    "    is_specialist = (active_authors > 0).sum(axis=1) == 1\n",
    "    specialist_authors = active_authors[is_specialist].index.tolist()\n",
    "    print(f\"Found {len(specialist_authors)} specialist authors.\")\n",
    "    return specialist_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b5321f8-4d57-4dd4-8eb6-4d93690e56fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Extracting Trail-Text Features (Top 1000)...\n",
      "\n",
      "--- Top Significant Trail-Words per Category ---\n",
      "Category: CULTURE\n",
      "latenight, hosts, trumps, new, discuss, discussed, weeks, shutdown, star, years\n",
      "Category: NEWS\n",
      "plus, read, years, data, days, need, pieces, climate, weekend, favourite\n",
      "Category: OPINION\n",
      "says, guardian, columnist, editorial, government, new, asks, writes, people, president\n",
      "Category: SPORT\n",
      "england, ashes, australia, test, win, blacks, englands, team, season, victory\n",
      "\n",
      "2. Extracting Tag Features (Top 500)...\n",
      "\n",
      "--- Top Significant Tags per Category ---\n",
      "Category: CULTURE\n",
      "culture, television, comedy, saturday, features, news, arts, music, article, radio\n",
      "Category: NEWS\n",
      "news, long, weather, read, weekly, guardian, main, section, features, crunch\n",
      "Category: OPINION\n",
      "opinion, comment, news, politics, matters, journal, article, trump, labour, donald\n",
      "Category: SPORT\n",
      "sport, cricket, team, rugby, australia, union, ashes, news, sports, newsletter\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\yuval\\\\Desktop\\\\לימודים\\\\ML\\\\models\\\\tfidf_tags.pkl']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- A. Trail Text Features ---\n",
    "print(\"\\n1. Extracting Trail-Text Features (Top 1000)...\")\n",
    "\n",
    "# התיקון: הוספנו stop_words='english' כדי להעיף את the, and, of\n",
    "trail_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "\n",
    "# אם את רוצה להוסיף גם את המילים הספציפיות שלך (כמו says), אפשר להעביר רשימה (אופציונלי)\n",
    "# אבל stop_words='english' יעשה את רוב העבודה הקשה\n",
    "X_trail = trail_vectorizer.fit_transform(df['trail_text'])\n",
    "\n",
    "# הצגת הניתוח (כדי לראות שהמילים הגיוניות עכשיו)\n",
    "analyze_top_features(df, trail_vectorizer, X_trail, \"Trail-Words\")\n",
    "\n",
    "trail_features = pd.DataFrame(\n",
    "    X_trail.toarray(), \n",
    "    columns=[f\"trail_{w}\" for w in trail_vectorizer.get_feature_names_out()]\n",
    ")\n",
    "joblib.dump(trail_vectorizer, MODEL_DIR / \"tfidf_trail.pkl\")\n",
    "\n",
    "# --- B. Tag Features ---\n",
    "print(\"\\n2. Extracting Tag Features (Top 500)...\")\n",
    "# גם כאן כדאי להוסיף ליתר ביטחון, למרות שבתגיות זה פחות קריטי\n",
    "tags_vectorizer = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "X_tags = tags_vectorizer.fit_transform(df['tags'])\n",
    "\n",
    "# הצגת הניתוח\n",
    "analyze_top_features(df, tags_vectorizer, X_tags, \"Tags\")\n",
    "\n",
    "tags_features = pd.DataFrame(\n",
    "    X_tags.toarray(), \n",
    "    columns=[f\"tag_{w}\" for w in tags_vectorizer.get_feature_names_out()]\n",
    ")\n",
    "joblib.dump(tags_vectorizer, MODEL_DIR / \"tfidf_tags.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "487d9d99-c9c4-439e-b729-d99bb90f293a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Extracting Specialist Author Features...\n",
      "\n",
      "Analyzing Author Specialization...\n",
      "Found 55 specialist authors.\n",
      "\n",
      "4. Combining all features...\n",
      "\n",
      "Success! Final dataset saved to: C:\\Users\\yuval\\Desktop\\לימודים\\ML\\dataset_features_final.csv\n",
      "Dataset Dimensions: (714, 1556)\n",
      " - Trail Text features: 1000\n",
      " - Tag features: 500\n",
      " - Author features: 55\n"
     ]
    }
   ],
   "source": [
    "if INPUT_FILE.exists():\n",
    "    # --- C: Author Features ---\n",
    "    print(\"\\n3. Extracting Specialist Author Features...\")\n",
    "    selected_authors = get_specialist_authors(df, min_articles=3)\n",
    "    joblib.dump(selected_authors, MODEL_DIR / \"authors_list.pkl\")\n",
    "    \n",
    "    author_features = pd.DataFrame()\n",
    "    for author in selected_authors:\n",
    "        col_name = f\"auth_{author.replace(' ', '_')}\"\n",
    "        author_features[col_name] = df['author'].apply(lambda x: 1 if x == author else 0)\n",
    "\n",
    "    # --- D: Combine and Save ---\n",
    "    print(\"\\n4. Combining all features...\")\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    trail_features.reset_index(drop=True, inplace=True)\n",
    "    tags_features.reset_index(drop=True, inplace=True)\n",
    "    author_features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    final_dataset = pd.concat([trail_features, tags_features, author_features], axis=1)\n",
    "    final_dataset['label'] = df['label']\n",
    "    \n",
    "    final_dataset.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"\\nSuccess! Final dataset saved to: {OUTPUT_FILE}\")\n",
    "    print(f\"Dataset Dimensions: {final_dataset.shape}\")\n",
    "    print(f\" - Trail Text features: {trail_features.shape[1]}\")\n",
    "    print(f\" - Tag features: {tags_features.shape[1]}\")\n",
    "    print(f\" - Author features: {author_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea28bdca-a287-4c49-b2bc-26d745a8dfbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
